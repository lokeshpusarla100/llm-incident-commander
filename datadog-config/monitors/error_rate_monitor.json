{
    "name": "[High Error Rate] LLM Incident Commander - Errors exceeding threshold",
    "type": "metric alert",
    "query": "avg(last_5m):(sum:llm.errors.total{service:llm-incident-commander}.as_count() / sum:llm.requests.total{service:llm-incident-commander}.as_count()) * 100 > 5",
    "message": "## ðŸš¨ High Error Rate Detected\n\nThe LLM Incident Commander is experiencing elevated error rates.\n\n**Current Metrics:**\n- Error rate: {{value}}% (threshold: 5%)\n- Service: {{service.name}}\n- Environment: {{env}}\n\n**Context:**\n- [View Error Logs](https://app.datadoghq.com/logs?query=service:llm-incident-commander%20status:error)\n- [View APM Errors](https://app.datadoghq.com/apm/services/llm-incident-commander/errors)\n\n**Common Error Types:**\n- `quota_exceeded`: Vertex AI quota limits hit\n- `timeout`: Requests exceeding timeout threshold\n- `api_error`: Vertex AI API failures\n\n**Runbook:**\n1. Check error logs for error types\n2. Verify Vertex AI service status\n3. Check quota limits in Google Cloud Console\n4. Review recent deployments or changes\n5. Validate authentication credentials\n\n**Next Steps:**\n- Identify primary error type\n- Implement mitigation (increase quota, add retry logic, etc.)\n- Monitor recovery\n\n@incident-llm-incident-commander @pagerduty",
    "tags": [
        "service:llm-incident-commander",
        "severity:critical",
        "category:availability"
    ],
    "options": {
        "thresholds": {
            "critical": 5,
            "warning": 2
        },
        "notify_no_data": false,
        "notify_audit": false,
        "require_full_window": true,
        "new_group_delay": 60,
        "include_tags": true,
        "escalation_message": "Error rate remains high. Immediate action required.",
        "renotify_interval": 30,
        "timeout_h": 0,
        "evaluation_delay": 60
    },
    "priority": 2,
    "restricted_roles": null
}