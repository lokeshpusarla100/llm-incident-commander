"""
Retrieval-Augmented Generation (RAG) Module.
Provides context for LLM responses based on incident knowledge base.
"""
from app.logging_config import setup_logging
from datadog import statsd
import time

logger = setup_logging()

# Incident knowledge base (production: use vector DB like Pinecone/Weaviate)
INCIDENT_KB = {
    "incident": """
An incident is a detected service disruption that impacts one or more users. 
Incidents have severity levels (SEV-1 to SEV-4), assigned owners, and timelines.
Key metrics: mean time to detection (MTTD), mean time to resolution (MTTR).
Incident response follows a standardized playbook with escalation procedures.
    """,
    
    "latency": """
Latency is the time elapsed between request submission and response receipt, measured in milliseconds.
P50 latency: median response time. P95 latency: 95th percentile. P99 latency: tail latency.
Typical LLM latency: 500-2000ms. Acceptable thresholds: P95 < 2000ms.
    """,
    
    "quota": """
API quota limits control resource consumption and prevent abuse.
Vertex AI quotas include: requests per minute (RPM), tokens per minute (TPM).
When quota is exhausted: API returns 429 Too Many Requests error.
Management: implement backpressure, rate limiting, priority queues.
    """,
    
    "hallucination": """
Hallucination is when an LLM generates false or unsupported information confidently.
Types: contradictions (against facts), unsupported claims (not in context).
Detection methods: semantic analysis, contradiction checking, entailment.
Mitigation: RAG, fact-checking, explicit prompting.
    """,
    
    "observability": """
Observability is the ability to understand system state from external outputs.
Three pillars: metrics (quantitative), logs (events), traces (request flow).
LLM observability: token usage, cost, latency, output quality, hallucinations.
Datadog provides: APM, custom metrics, structured logs, alerting.
    """,
    
    "token": """
Tokens are atomic units LLMs process. One token â‰ˆ 4 characters.
Input tokens: count in the prompt. Output tokens: generated by LLM.
Gemini 2.0 Flash: ~$0.075 per 1M input, ~$0.30 per 1M output tokens.
    """,
    
    "cost": """
LLM API costs: (input_tokens * input_rate + output_tokens * output_rate) / 1M.
Optimization: prompt caching, batching, cheaper models, quantization.
Datadog llm.cost.usd metric enables real-time cost monitoring.
    """,
    
    "datadog": """
Datadog is a monitoring and security platform for cloud applications.
It provides observability through metrics, logs, and traces.
LLM Observability features: token tracking, cost analysis, hallucination detection.
    """,
}


def retrieve_context(question: str) -> str:
    """
    Retrieve relevant context from incident knowledge base.
    Minimal RAG with keyword matching (production: semantic search).
    """
    start_time = time.time()
    question_lower = question.lower()
    relevant_docs = []
    
    # Keyword matching
    for keyword, doc in INCIDENT_KB.items():
        if keyword.lower() in question_lower:
            relevant_docs.append(doc.strip())
    
    context = "\n---\n".join(relevant_docs) if relevant_docs else ""
    retrieval_latency = (time.time() - start_time) * 1000
    
    # Emit metrics
    statsd.histogram("llm.retrieval.latency.ms", retrieval_latency)
    statsd.gauge("llm.retrieval.docs_retrieved", len(relevant_docs))
    
    if context:
        logger.debug("Context retrieved", extra={
            "keywords_matched": [k for k in INCIDENT_KB.keys() if k in question_lower],
            "docs_retrieved": len(relevant_docs)
        })
    
    return context


def get_knowledge_base_info() -> dict:
    """Get KB statistics for monitoring."""
    return {
        "total_docs": len(INCIDENT_KB),
        "keywords": list(INCIDENT_KB.keys()),
        "total_chars": sum(len(doc) for doc in INCIDENT_KB.values())
    }
