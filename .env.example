# =============================================================================
# LLM Incident Commander - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your values
# Run: cp .env.example .env && source .env
# =============================================================================

# -----------------------------------------------------------------------------
# Google Cloud Configuration (REQUIRED)
# -----------------------------------------------------------------------------
# Your Google Cloud Project ID - Application will not start without this
GCP_PROJECT_ID=your-gcp-project-id

# Vertex AI region (default: us-central1)
GCP_LOCATION=us-central1

# Gemini model to use (default: gemini-2.0-flash)
VERTEX_AI_MODEL=gemini-2.0-flash

# -----------------------------------------------------------------------------
# Datadog Configuration (REQUIRED for observability)
# -----------------------------------------------------------------------------
# Your Datadog API Key - Get from: Organization Settings > API Keys
DD_API_KEY=your-datadog-api-key

# Datadog site (datadoghq.com for US, datadoghq.eu for EU)
DD_SITE=datadoghq.com

# Service identification
DD_SERVICE=llm-incident-commander
DD_ENV=production
DD_VERSION=1.0.0

# Enable log injection for trace correlation
DD_LOGS_INJECTION=true

# -----------------------------------------------------------------------------
# Application Configuration (Optional)
# -----------------------------------------------------------------------------
APP_HOST=0.0.0.0
APP_PORT=8000

# LLM generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_OUTPUT_TOKENS=512
LLM_TIMEOUT_SECONDS=30

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Cost-Safety Controls (CRITICAL - Read carefully)
# -----------------------------------------------------------------------------
# SAFE_MODE: Disables all cloud infra assumptions (Vector Search, auto-LLM)
# Default: false (Gemini enabled by default, RAG disabled)
# Set to 'true' to block ALL costs.
SAFE_MODE=false

# DISABLE LLM generation by default - vector search only responds
# Set to 'true' ONLY when you explicitly need LLM reasoning
ENABLE_LLM_GENERATION=true

# =============================================================================
# üü¢ OPERATION MODES CHEAT SHEET
# =============================================================================
# 1. GEMINI ONLY (Default): SAFE_MODE=false, ENABLE_LLM_GENERATION=true, VECTOR_SEARCH_ENABLED=false
#    -> Runs LLM without context. Good for testing Gemini connection.
#
# 2. FULL RAG (Production): SAFE_MODE=false, ENABLE_LLM_GENERATION=true, VECTOR_SEARCH_ENABLED=true
#    -> Requires deployed Vector Search index. Full functionality.
#
# 3. ZERO COST (Safe Mode): SAFE_MODE=true
#    -> Blocks all external calls. Good for UI/trace testing.
# =============================================================================

# Similarity threshold: if RAG best_score >= this, return context directly (no LLM)
# Lower = more LLM calls, Higher = more vector-only responses
LLM_SIMILARITY_THRESHOLD=0.7

# Rate limit for Gemini calls per hour (per instance)
# Prevents runaway costs from traffic spikes
LLM_RATE_LIMIT_PER_HOUR=100

# Hard cap on output tokens (overrides per-request max_tokens)
# Protects against token explosion attacks
LLM_MAX_OUTPUT_TOKENS_CAP=1024

# Panic threshold: at this usage %, skip LLM and emit risk signal
# 0.9 = skip LLM when at 90% of rate limit
LLM_PANIC_THRESHOLD=0.9

# -----------------------------------------------------------------------------
# Vector Search / RAG Configuration (ON-DEMAND INFRASTRUCTURE)
# -----------------------------------------------------------------------------
# ‚ö†Ô∏è Vector Search uses per-hour billing when an index is deployed.
# We intentionally undeploy the endpoint when not in active use.
# This is standard enterprise practice for cost-responsible cloud usage.
#
# To enable Vector Search:
#   1. Run: python setup_vector_search.py
#   2. Wait ~5-10 minutes for index readiness
#   3. Set SAFE_MODE=false
#   4. Restart the application
#
# These are auto-created by setup_vector_search.py:
# VS_INDEX_ID=
# VS_ENDPOINT_ID=
# VS_BUCKET_NAME=
# VECTOR_SEARCH_ENABLED=true
