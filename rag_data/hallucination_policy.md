Hallucination occurs when LLM generates false information confidently.
Types: Contradictions (against facts), Unsupported claims (not in context).
Detection: Semantic analysis, contradiction checking, entailment.
Mitigation: RAG (retrieval-augmented generation), fact-checking, prompting.
